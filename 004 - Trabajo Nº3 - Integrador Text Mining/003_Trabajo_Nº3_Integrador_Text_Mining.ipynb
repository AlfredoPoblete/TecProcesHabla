{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrador Text Mining\n",
        "\n",
        "Este notebook aplicaremos minería de texto al contenido de críticas de películas extraído de un sitio web. El análisis se centrará en las películas mejor valoradas por los usuarios más influyentes, con el fin de descubrir información relevante, patrones o relaciones ocultas en los datos textuales."
      ],
      "metadata": {
        "id": "MYU9r974CZBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Importamos Librerias Necesarias"
      ],
      "metadata": {
        "id": "qt6fXouTPj1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeLCEkREA9ND"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
        "from collections import Counter\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descripcion de Librerias:\n",
        "\n",
        "* requests: Se utiliza para realizar peticiones HTTP a servidores web, como descargar el contenido HTML de una página.\n",
        "\n",
        "* BeautifulSoup: Biblioteca para analizar documentos HTML y XML. Permite navegar y buscar elementos específicos dentro de la estructura del documento de forma sencilla.\n",
        "\n",
        "* pandas: Proporciona estructuras de datos de alto rendimiento y fáciles de usar, como DataFrames, que son ideales para manipular y analizar datos tabulares (como los que se pueden extraer de una web).\n",
        "\n",
        "* re: Módulo para trabajar con expresiones regulares, que son patrones de búsqueda potentes para encontrar y manipular texto dentro de cadenas.\n",
        "\n",
        "* string: Contiene constantes útiles relacionadas con cadenas, como todos los caracteres de puntuación, letras, etc., que a menudo se utilizan para limpiar texto.\n",
        "\n",
        "* word_tokenize: Función de la librería Natural Language Toolkit (NLTK) para dividir una cadena de texto en palabras individuales (tokens).\n",
        "\n",
        "* CountVectorizer: De la librería scikit-learn, se utiliza para convertir una colección de documentos de texto en una matriz de conteo de tokens. Es un paso fundamental para aplicar modelos de aprendizaje automático a texto.\n",
        "\n",
        "* nltk: La librería Natural Language Toolkit, una suite completa de herramientas para el procesamiento del lenguaje natural, incluyendo tokenización, stemming, tagging, parsing y más.\n",
        "\n",
        "* stopwords, PlaintextCorpusReader: Módulos de NLTK. 'stopwords' proporciona una lista de palabras comunes en diferentes idiomas que suelen ser eliminadas en el análisis de texto. 'PlaintextCorpusReader' permite leer colecciones de archivos de texto sin formato como un corpus.\n",
        "\n",
        "* Counter: Un objeto de la librería estándar de Python que facilita el conteo de elementos en una lista o cualquier iterable. Es muy útil para contar la frecuencia de las palabras.\n",
        "\n",
        "* os: Módulo que proporciona una manera de interactuar con el sistema operativo, como crear directorios, manipular rutas de archivos, etc.\n",
        "\n",
        "* pikle: Pickle es un módulo en Python que permite serializar y deserializar objetos de Python.\n"
      ],
      "metadata": {
        "id": "Jolzu2J2QuuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Web Scraping\n",
        "\n",
        "Extraemos el contenido de las primeras 3 paginas web relacionada a criticas de 10 peliculas mejor valoradas por espectadores filtrados por usuarios mas seguidos."
      ],
      "metadata": {
        "id": "gpDrNvfeSF_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Buscamos las paginas almacenadas en la url filtrada"
      ],
      "metadata": {
        "id": "3Ch0Cat007oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos la url para luego extraer los diferentes links correspondientes a las 10 peliculas\n",
        "url = \"https://www.sensacine.com/peliculas/mejores-peliculas/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "for a in soup.find_all(\"a\", href=True, class_=\"meta-title-link\"):\n",
        "  print(a)"
      ],
      "metadata": {
        "id": "YEfAwcticilJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que cada uno de los links se encuentran incompletos, por lo cual agregamos a cada link la extensión que corresponde para poder ingresar a cada una de ellas"
      ],
      "metadata": {
        "id": "29U-Z7gX9WZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteramos para cada etiqueta para agregar las extenciones correspondientes de la pagina\n",
        "for a in soup.find_all(\"a\", href=True, class_=\"meta-title-link\"):\n",
        "  link = a['href']\n",
        "  print(\"https://www.sensacine.com\" + link + \"criticas-espectadores/mas-seguidos/\")"
      ],
      "metadata": {
        "id": "2km_6a7Xc9kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Almacenamos el contenido de las primeras 3 paginas de cada uno de los links"
      ],
      "metadata": {
        "id": "W8ympFK5MRP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea listas vacias para almacenar el titulo de cada pelicula en una lista cada una de las criticas en otra lista.\n",
        "titulo = []\n",
        "critica = []\n",
        "\n",
        "# Recorre cada url para almacenar las primeras 3 url correspondientes a criticas de usuarios mas seguidos\n",
        "for a in soup.find_all(\"a\", href=True, class_=\"meta-title-link\"):\n",
        "  link = a['href']\n",
        "  pag_1 = \"https://www.sensacine.com\" + link + \"criticas-espectadores/mas-seguidos/\"\n",
        "  texto_1 = requests.get(pag_1)\n",
        "  soup_1 = BeautifulSoup(texto_1.content, \"html.parser\")\n",
        "  pag_2 = pag_1 + \"?page=2\"\n",
        "  texto_2 = requests.get(pag_2)\n",
        "  soup_2 = BeautifulSoup(texto_2.content, \"html.parser\")\n",
        "  pag_3 = pag_1 + \"?page=3\"\n",
        "  texto_3 = requests.get(pag_3)\n",
        "  soup_3 = BeautifulSoup(texto_3.content, \"html.parser\")\n",
        "\n",
        "  # Extrae el titulo y lo guarda en una variable para ser almacenada en la lista\n",
        "  titulo.append(a.text)\n",
        "\n",
        "  # Extrae el contenido de las 3 primeras paginas de cada pelicula\n",
        "  content1 = soup_1.find_all(\"div\", class_=\"content-txt review-card-content\")\n",
        "  content2 = soup_2.find_all(\"div\", class_=\"content-txt review-card-content\")\n",
        "  content3 = soup_3.find_all(\"div\", class_=\"content-txt review-card-content\")\n",
        "\n",
        "  # Si existe contenido en cada contenedor lo almacenamos\n",
        "  content = \"\"\n",
        "  if content1:\n",
        "    content += \"\\n\".join([c.get_text(strip=True) for c in content1])\n",
        "  if content2:\n",
        "    content += \"\\n\".join([c.get_text(strip=True) for c in content2])\n",
        "  if content3:\n",
        "    content += \"\\n\".join([c.get_text(strip=True) for c in content3])\n",
        "  critica.append(content)\n"
      ],
      "metadata": {
        "id": "yzgImEwXUQEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Creamos un dataframe con los datos almacenados"
      ],
      "metadata": {
        "id": "_O7uJa9-R233"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [titulo, critica]\n",
        "df_original = pd.DataFrame(data, index=['titulo', 'critica']).transpose()\n",
        "df_original.head(10)"
      ],
      "metadata": {
        "id": "llK2Azp9megJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el dataframe en formato pikle\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Creamos el directorio si no existe\n",
        "directory_path = '/content/drive/MyDrive/TP_Proces_Habla'\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "df_original.to_pickle('/content/drive/MyDrive/TP_Proces_Habla/df_original.pkl')\n"
      ],
      "metadata": {
        "id": "k8nuIsMd_GTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Limpieza de Datos\n",
        "Para poder trabajar de manera más efectiva y extraer información significativa\n",
        "limpiamos el texto de expresiones regulares"
      ],
      "metadata": {
        "id": "B0mpUguWBZrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Creamos una funcion para eliminar expresiones regulares del texto"
      ],
      "metadata": {
        "id": "0_IMsPdjjSoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino la funcion si el texto existe en cada registro\n",
        "def limpiar_texto(text):\n",
        "  if text is not None:\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…«»]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "  else:\n",
        "        text = ''\n",
        "  return text"
      ],
      "metadata": {
        "id": "awaaVi3yL-ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Aplicamos la funcion a un nuevo dataframe"
      ],
      "metadata": {
        "id": "YBdFXoQSj6M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero copiamos eldataframe original y luego aplicamos la funcion\n",
        "df_limpio  = df_original.copy()\n",
        "df_limpio['critica'] = df_limpio['critica'].apply(limpiar_texto)\n",
        "df_limpio.head()\n"
      ],
      "metadata": {
        "id": "4peXvBLwPT3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Guardamos el dataframe"
      ],
      "metadata": {
        "id": "x-BYCf_nkNjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el dataframe en formato pikle\n",
        "df_limpio.to_pickle('/content/drive/MyDrive/TP_Proces_Habla/df_limpio.pkl')"
      ],
      "metadata": {
        "id": "V2vnHgbxrXPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carpeta_destino = '/content/drive/MyDrive/TP_Proces_Habla/Criticas_por_Peliculas'\n",
        "\n",
        "# Crea la carpeta si no existe\n",
        "if not os.path.exists(carpeta_destino):\n",
        "    os.makedirs(carpeta_destino)\n",
        "    print(f\"Carpeta '{carpeta_destino}' creada.\")\n",
        "else:\n",
        "    print(f\"La carpeta '{carpeta_destino}' ya existe.\")\n",
        "\n",
        "# Itera sobre cada fila del DataFrame\n",
        "for index, row in df_limpio.iterrows():\n",
        "    titulo = row['titulo']\n",
        "    critica_texto = row['critica']\n",
        "\n",
        "    # Crea un nombre de archivo seguro a partir del título\n",
        "    nombre_archivo = f\"{''.join(c if c.isalnum() else ' ' for c in titulo)}.txt\"\n",
        "    ruta_archivo = os.path.join(carpeta_destino, nombre_archivo)\n",
        "\n",
        "    try:\n",
        "        # Guarda la crítica en un archivo de texto\n",
        "        with open(ruta_archivo, 'w', encoding='utf-8') as archivo:\n",
        "            archivo.write(critica_texto)\n",
        "        print(f\"Crítica de '{titulo}' guardada en: {ruta_archivo}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar la crítica de '{titulo}': {e}\")\n",
        "\n",
        "print(\"Proceso de guardado completado.\")"
      ],
      "metadata": {
        "id": "Xc-gSpqCkU2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Creamos nuestro “Bag of Words”\n",
        "\n",
        "Creamos una bolsa de palabras para representar una version simplificada de cada una de las criticas como vector, con el objetivo de estudiar las palabras que más frecuentes."
      ],
      "metadata": {
        "id": "okdmI2u9rnNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos stopwords en spanish\n",
        "nltk.download('stopwords')\n",
        "lines = nltk.corpus.stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "3pi3C-E-Op1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir una colección de documentos de texto en una matriz de términos/tokens, ignorando stop words\n",
        "cv = CountVectorizer(stop_words=lines)\n",
        "\n",
        "# Ajustamos el modelo y lo aplicamos al texto de nuestro dataframe generando una matriz dispersa\n",
        "df_cv = cv.fit_transform(df_limpio['critica'])\n",
        "\n",
        "# Guardamos el objeto CountVectorize\n",
        "pickle.dump(cv, open(\"/content/drive/MyDrive/TP_Proces_Habla/cv.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "Bo2t4feFSwao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierte la matriz dispersa a una matriz densa y la estructura en un DataFrame\n",
        "df_dtm = pd.DataFrame(df_cv.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Le asignamos los indices del dataframe\n",
        "df_dtm.index = df_limpio['titulo']\n",
        "\n",
        "# Guardamos el dataframe como matriz esparsa en formato pickle\n",
        "df_dtm.to_pickle('/content/drive/MyDrive/TP_Proces_Habla/df_dtm.pkl')"
      ],
      "metadata": {
        "id": "1IBlXAxi3f9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dtm"
      ],
      "metadata": {
        "id": "P-tyKguEWyoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta representacion de texto a matriz esparsa nos permie realizar análisis más complejos, ahorrando espacio y mejorando el rendimiento en operaciones matematicas."
      ],
      "metadata": {
        "id": "vToR3cYm59dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Analisis Exploratorio\n"
      ],
      "metadata": {
        "id": "JClpe1JI5_Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Exploramos el dataset"
      ],
      "metadata": {
        "id": "Ooz6jzgjJ7t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos el dataframe de manera transpuesta\n",
        "dtm_transpuesta = df_dtm.transpose()\n",
        "dtm_transpuesta"
      ],
      "metadata": {
        "id": "3Fhw5OCgY9Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Palabras mas usadas por pelicula"
      ],
      "metadata": {
        "id": "qQC9RUiuxoxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteramos en cada registro\n",
        "for index, row in df_limpio.iterrows():\n",
        "    titulo = row['titulo']\n",
        "    critica = row['critica']\n",
        "\n",
        "    # Transformar cada texto en un vector de conteo\n",
        "    X = cv.fit_transform([critica])  # Transform the single review\n",
        "\n",
        "    # Obtiene la palabra y su frecuencia en cada una de las criticas\n",
        "    words = cv.get_feature_names_out()\n",
        "    frequencias = X.toarray().sum(axis=0)\n",
        "\n",
        "    # Crea un diccionario y ordena las palafras con mayor frecuencia\n",
        "    word_freq = dict(zip(words, frequencias))\n",
        "    frecuencia_ordenada = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Muestra el titulo de la pelicula con las precuencias de cada palabra\n",
        "    print(f\"'{titulo}': {[(word, freq.item()) for word, freq in frecuencia_ordenada]}\")"
      ],
      "metadata": {
        "id": "XsAsbpHI4C-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo un diccionario\n",
        "top_dict = {}\n",
        "\n",
        "# Itera por cada filas y muestro en forma decreciente las 30 palabras mas usadas por pelicula\n",
        "for c in dtm_transpuesta.columns:\n",
        "    top = dtm_transpuesta[c].sort_values(ascending=False).head(30) #\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "\n",
        "print(top_dict)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Imprimo las 15 palabras mas frecuentes por pelicula\n",
        "for titulo, top_words in top_dict.items():\n",
        "    print(titulo)\n",
        "    print(', '.join([str(word) for word, count in top_words[0:14]]))"
      ],
      "metadata": {
        "id": "-e9mC7_AwtQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notamos que entre las palabras mas usadas en cada critica, se encuentran aquellas que no nos aporta informacion importante, lo cual trataremos a continuacion"
      ],
      "metadata": {
        "id": "tgZhDWeA9iKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Agregamos palabras redundantes a stopwords\n",
        "\n"
      ],
      "metadata": {
        "id": "HexTaK_e-VUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Armo una lista con cada palabra\n",
        "words = []\n",
        "for titulo in dtm_transpuesta.columns:\n",
        "    top = [word for (word, count) in top_dict[titulo]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "\n",
        "# Con .most_common() obtengo las palabras mas usadas, y la cantidad de veces que fue agregada a la lista words\n",
        "print(Counter(words).most_common())\n",
        "\n",
        "# Creo una lista de nuevas stop words\n",
        "add_stop_words = [word for word, count in Counter(words).most_common() if count > 5]\n",
        "print(add_stop_words)"
      ],
      "metadata": {
        "id": "c25NCGAn-He3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregamos la lista de stop words \"lines\" las nuevas stop words\n",
        "for pal in add_stop_words:\n",
        "    lines.append(pal)\n",
        "\n",
        "# Creamos la lista que tiene algunos nombres propios que aparecieron entre las palabras mas frecuentes y otro tipo de palabras\n",
        "more_stop_words=['aser','forrest','gump','hace','dos','pulp','primera','secuela','fiction','pacino','ver','the','vez','si','parte','film','va','hacer','veces', 'así', 'luego', 'quizá','cosa','cosas','tan','asi','disney','todas','sólo','corleone','spoiler','schindler','aaron','pelicula','padrino','rey','león']\n",
        "for pal in more_stop_words:\n",
        "    lines.append(pal)\n",
        "\n",
        "# Recreamos la matriz de documentos y terminos pero ustando la nueva lista \"mejorada\" de stopwords\n",
        "cv = CountVectorizer(stop_words=lines)\n",
        "data_cv = cv.fit_transform(df_limpio['critica'])\n",
        "# Use get_feature_names_out() instead of get_feature_names()\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
        "data_stop.index = df_limpio.index"
      ],
      "metadata": {
        "id": "DML7N5ZBBFyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_stop"
      ],
      "metadata": {
        "id": "Z-tVZon_G3uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera obtenemos una matriz reducida de palabras innecesarias"
      ],
      "metadata": {
        "id": "PGveV_pALr0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Nubes de palabras\n",
        "\n",
        "Creamos una nube de palabras por cada pelicula"
      ],
      "metadata": {
        "id": "YqkH1YaLMFbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure plot size\n",
        "plt.rcParams['figure.figsize'] = [12, 18]\n",
        "\n",
        "# Create WordCloud object\n",
        "wordcloud = WordCloud(\n",
        "    stopwords=lines,\n",
        "    background_color=\"lightblue\",\n",
        "    colormap=\"Dark2\",\n",
        "    max_font_size=150,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Generate and display word clouds for each movie title\n",
        "num_plots = len(df_limpio['titulo'])\n",
        "num_rows = 5\n",
        "num_cols = 2\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))  # Create subplots with specified size\n",
        "\n",
        "for index, (titulo, critica) in enumerate(zip(df_limpio['titulo'], df_limpio['critica'])):\n",
        "    # Only plot if within the subplot grid\n",
        "    if index < num_rows * num_cols:\n",
        "        # Calculate subplot row and column index\n",
        "        row_index = index // num_cols\n",
        "        col_index = index % num_cols\n",
        "\n",
        "        # Generate word cloud\n",
        "        wordcloud.generate(critica)\n",
        "\n",
        "        # Display word cloud in the corresponding subplot\n",
        "        axes[row_index, col_index].imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        axes[row_index, col_index].axis(\"off\")\n",
        "        axes[row_index, col_index].set_title(titulo)\n",
        "\n",
        "plt.tight_layout()  # Adjust subplot parameters for a tight layout\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zX67wEjwKpxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con estos graficos podemos sacar diferentes conclusiones:\n",
        "\n",
        "*  La presencia de actores o personajes memorables\n",
        "*  Se comenta mas el tema de la familia en El padrino 2 que en la 1\n",
        "*  La animacion de la pelicula el Rey Leon es muy significativa\n",
        "*  Se habla mas del villano, que del heroe en la pelicula de Batman"
      ],
      "metadata": {
        "id": "JwjbgGEcNGzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6 - Frecuencia de Palabras Clave en Críticas de Películas (Por Aspecto)"
      ],
      "metadata": {
        "id": "YeSx7IwDnDs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root = '/content/drive/MyDrive/TP_Proces_Habla/Criticas_por_Peliculas'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
        "#wordlists.fileids() # con esto listamos los archivos del directorio\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "        (word,genre)\n",
        "        for genre in df_limpio['titulo']\n",
        "        for w in wordlists.words(''.join(c if c.isalnum() else ' ' for c in genre) + '.txt')  # Fix: Use the same file naming logic\n",
        "        for word in ['historia','efectos','trama','sonora']\n",
        "        if w.lower().startswith(word) )\n",
        "cfd.plot()\n",
        "\n",
        "# Agregar título a la imagen\n",
        "plt.title('¿De que mas se habla en cada pelicula?', fontsize=16)\n",
        "\n",
        "# Obtener el objeto AxesSubplot para configurar los nombres de los ejes\n",
        "ax = plt.gca()\n",
        "\n",
        "# Nombre del eje x (condiciones)\n",
        "ax.set_xlabel('Peliculas', fontsize=12)\n",
        "\n",
        "# Nombre del eje y (frecuencia)\n",
        "ax.set_ylabel('Frecuencia', fontsize=12)\n",
        "\n",
        "plt.tight_layout() # Ajusta el layout para evitar que las etiquetas se superpongan\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GSR-kTjvm_hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queda mas que claro que se habla mas de la historia de la pelicula y como segundo la banda sonora"
      ],
      "metadata": {
        "id": "jogbde2yrWKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 - Creamos un nuevo Dataframe\n",
        "\n",
        "En este apartado crearemos un nuevo dataframe con valores numericos para descubrir informacion importante"
      ],
      "metadata": {
        "id": "gNAbZJYgRhZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Creamos una columna que representa el numero de palabras unicas por cada pelicula"
      ],
      "metadata": {
        "id": "tNY9aRdcV76j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontraremos las palabras unicas por peliculas identificando los elementos non-zero en la matriz de documentos y terminos\n",
        "data = df_dtm.transpose().copy()\n",
        "unique_list = []\n",
        "for titulo in data.columns:\n",
        "    uniques = data[titulo].to_numpy().nonzero()[0].size # tengo que transformar a un array para aplicar la funcion nonzero\n",
        "    unique_list.append(uniques)\n",
        "\n",
        "# Creo un nuevo dataframe con el numero de palabras unicas por pelicula\n",
        "data_words = pd.DataFrame(list(zip(df_limpio['titulo'], unique_list)), columns=['Pelicula', 'Palabras Unicas'])\n",
        "data_unique_sort = data_words.sort_values(by='Palabras Unicas', ascending=False)\n",
        "print(\"Top criticas de peliculas con mas palabras unicas.\\n\")\n",
        "print(data_unique_sort)"
      ],
      "metadata": {
        "id": "qLhaO1yYqkpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Web Scraping\n",
        "\n",
        "Extraemos valores numericos de la pagina web como el puntaje de cada pelicula y el numero de notas para añadirla al nuevo dataset"
      ],
      "metadata": {
        "id": "a-MkkDClWbBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_criticas = []\n",
        "puntaje_usuarios = []\n",
        "num_notas = []\n",
        "puntaje_medios = []\n",
        "# Suponiendo que 'soup' es el objeto BeautifulSoup con el contenido de la página principal\n",
        "for a in soup.find_all(\"a\", href=True, class_=\"meta-title-link\"):\n",
        "    link = a['href']\n",
        "    link_medio = \"https://www.sensacine.com\" + link\n",
        "    texto_medio = requests.get(link_medio)\n",
        "    bsoup_medio = BeautifulSoup(texto_medio.content, \"html.parser\")\n",
        "    puntaje_medio = bsoup_medio.find(\"span\", class_=\"stareval-note\")\n",
        "    if puntaje_medio:\n",
        "      puntaje_medios.append(puntaje_medio.get_text())\n",
        "    link_title = \"https://www.sensacine.com\" + link + \"criticas-espectadores/mas-seguidos/\"\n",
        "    texto_title = requests.get(link_title)\n",
        "    bsoup_critic = BeautifulSoup(texto_title.content, \"html.parser\")\n",
        "    score = bsoup_critic.find(\"span\", class_=\"note\")\n",
        "    n_note = bsoup_critic.find(\"span\", class_=\"user-note-count\")\n",
        "    n_critic = bsoup_critic.find(\"h2\", class_=\"titlebar-title titlebar-title-md\")\n",
        "    if n_critic:\n",
        "      n_criticas = int(n_critic.get_text().split()[0])\n",
        "      num_criticas.append(n_criticas)\n",
        "    if score:\n",
        "      puntaje_usuarios.append(score.get_text())\n",
        "    if n_note:\n",
        "      num_nota = int(n_note.get_text().split()[0])\n",
        "      num_notas.append(num_nota)\n",
        "\n"
      ],
      "metadata": {
        "id": "N-CLBzUDYkIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agrego las columnas al dataframe\n",
        "df_num = data_words.copy()\n",
        "df_num['N° Criticas'] = num_criticas\n",
        "df_num['N° Notas'] = num_notas\n",
        "df_num['Puntaje Usuarios'] = pd.to_numeric(pd.Series(puntaje_usuarios).str.replace(',', '.'))\n",
        "df_num['Puntaje Medios'] = pd.to_numeric(pd.Series(puntaje_medios).str.replace(',', '.'))\n",
        "\n",
        "df_num"
      ],
      "metadata": {
        "id": "BKB7E89dv2pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Graficamos la frecuencia de cada variable"
      ],
      "metadata": {
        "id": "QSd66JWgXxcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [18, 6]\n",
        "fig = plt.figure()  # Creamos la figura para el título general\n",
        "\n",
        "y_pos = np.arange(len(df_num))\n",
        "\n",
        "# Subplot 1: Número de palabras únicas\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "bars1 = ax1.barh(y_pos, df_num['Palabras Unicas'], align='center', color='darkgreen')\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(df_num['Pelicula'])\n",
        "ax1.set_title('N° de palabras unicas', fontsize=14)\n",
        "ax1.set_xlabel('Cantidad')  # Nombre del eje x para el primer gráfico\n",
        "for bar in bars1:\n",
        "    width = bar.get_width()\n",
        "    ax1.annotate(f'{width}', xy=(width / 2, bar.get_y() + bar.get_height() / 2),\n",
        "                ha='center', va='center', color='white', fontsize=8) # Centrar y cambiar color\n",
        "\n",
        "# Subplot 2: Número de críticas\n",
        "ax2 = fig.add_subplot(1, 3, 2)\n",
        "bars2 = ax2.barh(y_pos, df_num['N° Criticas'], align='center', color='darkgreen')\n",
        "ax2.set_yticks(y_pos)\n",
        "ax2.set_yticklabels(df_num['Pelicula'])\n",
        "ax2.set_title('Numero de criticas', fontsize=14)\n",
        "ax2.set_xlabel('Cantidad')  # Nombre del eje x para el segundo gráfico\n",
        "for bar in bars2:\n",
        "    width = bar.get_width()\n",
        "    ax2.annotate(f'{width}', xy=(width / 2, bar.get_y() + bar.get_height() / 2),\n",
        "               ha='center', va='center', color='white', fontsize=8) # Centrar y cambiar color\n",
        "\n",
        "# Subplot 3: Número de notas\n",
        "ax3 = fig.add_subplot(1, 3, 3)\n",
        "bars3 = ax3.barh(y_pos, df_num['N° Notas'], align='center', color='darkgreen')\n",
        "ax3.set_yticks(y_pos)\n",
        "ax3.set_yticklabels(df_num['Pelicula'])\n",
        "ax3.set_title('N° de notas', fontsize=14)\n",
        "ax3.set_xlabel('Cantidad')  # Nombre del eje x para el tercer gráfico\n",
        "for bar in bars3:\n",
        "    width = bar.get_width()\n",
        "    ax3.annotate(f'{width}', xy=(width / 2, bar.get_y() + bar.get_height() / 2),\n",
        "                ha='center', va='center', color='white', fontsize=8) # Centrar y cambiar color\n",
        "\n",
        "fig.suptitle('Análisis de Películas', fontsize=16)  # Título para los tres gráficos\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajusta el layout para que no se superponga el título principal\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FHFDpFEZHdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gracias a este grafico concluimos lo siguiente:\n",
        "* Aunque la pelicula de El padrino 2 tiene menor numero de criticas, obtiene el segundo lugar en el top de numero de palabras unicas\n",
        "* Las peliculas: Batman el caballero oscuro y El padrino, son las peliculas que aproximadamente tienen mayor frecuencia de cada una de las variables"
      ],
      "metadata": {
        "id": "gqbNfHq9s1Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Grafico de Puntaje por pelicula"
      ],
      "metadata": {
        "id": "B07sWvm2vDmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_num[['Pelicula','Puntaje Usuarios', 'Puntaje Medios']]"
      ],
      "metadata": {
        "id": "gnoTYSGOwTfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ancho de las barras\n",
        "ancho = 0.35\n",
        "x = np.arange(len(df_num['Pelicula']))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "rects1 = ax.bar(x - ancho/2, df_num['Puntaje Medios'], ancho, label='Puntaje Medios', color='sienna')\n",
        "rects2 = ax.bar(x + ancho/2, df_num['Puntaje Usuarios'], ancho, label='Puntaje Usuarios', color='goldenrod')\n",
        "\n",
        "# Etiquetas y títulos\n",
        "ax.set_ylabel('Puntaje')\n",
        "ax.set_xlabel('Película')\n",
        "ax.set_title('Comparación de Puntajes por Película')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_num['Pelicula'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCbhznZH4nsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizando el grafico notamos:\n",
        "* En 7 de las 10 peliculas el puntaje de usuarios es mayor al de los medios especializados\n",
        "* La pelicula con mayor puntaje es de 'Batman el caballero de la noche' segun Medios y segun usuarios 'El Padrino'\n",
        "* La pelicula con menor puntaje es de 'El gladiador' segun Medios y segun usuarios 'Pulp Fiction'"
      ],
      "metadata": {
        "id": "q5-YY3ofveNn"
      }
    }
  ]
}